---
title             : "Active Choice Hypothesis Testing: Example Manuscript"
shorttitle        : "Active Choice Hypothesis Testing"

author: 
  - name          : "Shilaan Alzahawi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "655 Knight Way, Graduate School of Business, Stanford, CA 94305"
    email         : "shilaan@stanford.edu"

affiliation:
  - id            : "1"
    institution   : "Stanford University, Graduate School of Business"
    
note: "\\clearpage"

authornote:

abstract: 
  
keywords          : "Hypothesis Testing"
wordcount         : "2,860"

bibliography      : ["references.bib", "r-references.bib"]

floatsintext      : yes #place figures and tables in the text rather than at the end
numbersections    : no #number sections headings
figurelist        : no #create list of figure captions
tablelist         : no #create list of table captions
footnotelist      : no ##create list of footnotes
linenumbers       : no #add line numbers in the margins
mask              : no #omit identifying information from the title page
draft             : no #add "draft" watermark across all pages

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library(ggplot2)
library(papaja)
library(citr)
library(rpact)
library(dplyr)
r_refs("r-references.bib")
```

"Whatever else is done about null-hypothesis tests, let us stop viewing statistical analysis as a sanctification process. We are awash in a sea of uncertainty, caused by a flood tide of sampling and measurement errors, and there are no objective procedures that avoid human judgment and guarantee correct interpretations of results." [@abelson1997: p.13]

Psychological science is experiencing a period of widespread methodological reflection [@nelson2018]. Starting 2010, a chain of events caused psychological scientists to doubt the credibility of psychological research [@moshontz2018]. These events included a case of widespread academic fraud, a series of failed replications of famous findings, and a paper published in psychology's most prestigious journal claiming that extrasensory perception (ESP) is real [@nelson2018]. 

According to some, one of the root causes of the crisis of confidence is the misunderstanding and misuse of statistical methods [@goodman2019]. In psychological science, one of the most frequently used approaches to statistical inference is the null-hypothesis significance test (NHST). For decades, researchers have been warned about mindless applications of this tool [e.g., @cohen1994; @gigerenzer2004; @nickerson2000; @rozeboom1960].

It has been argued that, in practice, the NHST has taken on a mechanical nature. According to Gigerenzer et al. [-@gigerenzer1989, p.210-227], scientists have interpreted the NHST as a completely mechanized procedure to arrive at new knowledge while eliminating the need for personal judgment and promising objectivity. A normative view has emerged; statistics can speak with one voice and replace personal judgment by means of formal rules. 

Among statisticians, there seems to be widespread agreement about what not do with hypothesis tests and *p*-values. Unfortunately, there is less agreement about what researchers should be doing instead [@wasserstein2019]. Crucially, although the founders of statistics disagreed on many things, they seemed to agree on at least one thing: they all rejected mechanical inference and highlighted the importance of human judgment [@abelson1995; @gigerenzer2015; @mayo2018].

Unfortunately, the literature on NHST seems to have lost sight of the important distinction between the tool itself and its thoughtless application [@mayo2018; @lakens2020a]. It seems the tool has become synonymous with its weakest construal: setting up a nil-null hypothesis and using a single *p*-value below a conventional threshold to infer evidence for a substantive research claim [@mayo2018; @gelman2016]. As a consequence, numerous scholars have called for banning the tool altogether. 

It is important to emphasize, however, that NHST has a place in scientific inference [@abelson1997; @lakens2020a] and that it is likely to stay [@goodman2019]. Rather than invalidating the tool on the basis of its weakest construal, an important step forward is to highlight how NHSTs can be thoughtfully applied in practice [@wasserstein2019]. 

In this paper, I examine the way in which hypothesis tests are conducted in psychological science. I focus on three decisions central to hypothesis testing: (a) the null hypothesis to be tested, (b) the choice of acceptable error rates, and (c) the determination of sample size. I refer to conventional uses of NHST (i.e., choices that are widely shared across the literature, are made without justification, and/or refer to heuristics or conventions) as *defaults*. I refer to thoughtful uses of NHST (i.e., choices that are actively justified) as *active choices*. In addition to examining common practice, I provide a practical overview of active choices that can be made within the traditional framework to strengthen the practice of null-hypothesis significance testing. 

In the first section, I introduce default hypothesis tests and their thoughtful alternatives. In the second section, I conduct a literature review to examine the ways in which hypothesis tests are practiced in the psychological research literature. My goal is to assess the choices psychological scientists make when testing a hypothesis, the extent to which justifications are provided for these choices, and the extent to which choices are shared across the research literature (i.e., the prevalence of conventions). Of special interest are the choice of the null hypothesis (i.e., whether to test a nil-null or, for example, a range prediction), the choices of error rates (i.e., how the balance is struck between type I and type II error rates), and the determination of sample size.  

In the third section, I provide an in-depth, practical introduction to active choice hypothesis tests: a selection of thoughtful choices that can be made within the traditional framework to strengthen the practice of null-hypothesis significance testing. First, I consider the choice of null hypothesis. As an alternative to testing nil-null hypotheses, I introduce several approaches researchers can take to test range predictions. 

Second, I consider the choice of error rates. As an alternative to using conventional error thresholds, I discuss several approaches that researchers can take to choose and justify their error rates. Third, I consider sample size determination. As an alternative to using heuristics and unrealistic power estimates (based on effect sizes researchers deem plausible), I discuss several approaches that researchers can take to justify their sample size. In addition, I present a tool that allows researchers to efficiently perform well-powered experiments: sequential analysis. 

# Default vs. Active Choice Hypothesis Tests

"Knowing what not to do with *p*-values is indeed necessary, but it does not suffice. It is as though statisticians were asking users of statistics to tear out the beams and struts holding up the edifice of modern scientific research without offering solid construction materials to replace them. Pointing out old, rotting timbers was a good start, but now we need more." [@wasserstein2019: p. 1]

## On the choice of the null hypothesis
"This type of problem... is the basis of the tests of significance by which we can examine whether or not the data are in harmony with any suggested hypothesis." [@fisher1925: p. 8]


The meaning of 'null' hypothesis is commonly misunderstood [@bakan1967]. In the weakest construal of the NHST, the null hypothesis tested is a hypothesis of no difference [i.e., an effect or difference of exactly zero; @mayo2018]; this null is commonly referred to as the *nil-null* or the *straw-man* null hypothesis. However, the term null was originally used to refer to any hypothesis to be 'nullified' [@bakan1967; @mayo2018]. 

The null can be any hypothesis a researcher wishes to test. According to Neyman [-@neyman1957: p.10], envisioning possible hypotheses that are relevant to the phenomenon of interest is the work of creative imagination. Many of the problems associated with NHST lessen if the null hypothesis is defined as something other than a hypothesized difference, effect, or correlation of precisely zero [@nickerson2000]. 

A stronger construal of the NHST is one where the researcher tests a range null hypothesis rather than a nil-null hypothesis [@nickerson2000; @lakens2020a]: a move, referred to in the title, from a straw man to a steel woman. Instead of testing the nil-null hypothesis that the effect is exactly zero, researchers can test whether effects more extreme than a range of predicted or practically important effect sizes can be rejected. Examples of such tests are equivalence tests, minimum-effect tests, and inferiority tests [@lakens2020a; @lakens2018]. In the third section, I will give a practical introduction to the use of range predictions in hypothesis tests. 

## On the choice of error rates

"[N]o scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas." [@fisher1956: p. 44-45]

"From the point of view of mathematical theory all that we can do is to show how the risk of the errors may be controlled and minimized. The use of these statistical tools in any given case, in determining just how the balance should be struck, must be left to the investigator." [@neyman1933: p.293]

Prior to conducting a Neyman-Pearson hypothesis test, researchers need to decide on a type I error rate (i.e., the long-run rate at which a true null hypothesis is rejected) and a type II error rate (i.e., given a number of participants, the long-run probability of failing to reject the null for a specified effect size of interest). In effect, a researcher needs to decide on the importance of exercising scientific caution (i.e., avoiding a type I error) versus the importance of discovery of an effect (i.e., avoiding a type II error). 

However, in practice, the sense of balancing the two types of error has been lost [@oakes1986; @rozeboom1960]. More commonly, researchers tend to use conventional cut-offs for the two error rates ($\alpha$ =.05 and $\beta$ = .20, implying that type I errors are considered 4 times more costly than type II errors). In recent work, researchers have been urged to justify their $\alpha$ [@lakens2018]. In the third section, I will elaborate on several approaches to the choice and justification of error rates. 

## On the choice of sample size

"[I]t is an essential characteristic of experimentation that it is carried out with limited resources, and an essential part of the subject of experimental design to ascertain how these should be best applied." [@fisher1935: p. 18]

As early as 1962, the perils of conducting small-sample, low-power experiments were introduced to the psychological literature [@cohen1962]. Unfortunately, more recent work has not been able to paint a rosier picture of the statistical power of hypothesis tests in psychological science [@bakker2012; @morey2016]. 

A possible reason that hypothesis tests often have low chances of detecting meaningful effects is that researchers use heuristics (e.g., *N* = 50 per cell) or effect size estimates from previous studies (which may very well be inflated) when determining sample size [@lakens2017]. Alternative approaches that have been suggested are safeguard power analysis [@perugini2014], expected power analysis [@biesanz2017], and powering the test to detect the smallest effect size of interest [@lakens2017].

Researchers are often surprised to find out how many observations are typically required for a test to have appropriate power. Sequential analysis, on the other hand, allows researchers to perform well-powered experiments with smaller sample sizes [@wald1973]. Thus, sequential analysis has the potential to significantly improve the practice of hypothesis testing in psychological science [@beffarabret2018; @lakens2014; @schnuerch2020]. In the following section, I give a hands-on introduction to sequential hypothesis testing.  

## *Sequential Analysis*

In 1929, Harold Dodge and Harry Romig introduced the idea of sequential analysis to the science of statistical quality control [@dodge1929]. Sequential analysis is a method of statistical inference that allows data to be analyzed several times during its collection, with the goal of optimizing the amount of resources spent in research [@beffarabret2018]. When Abraham Wald further developed this idea in the '40s, his work was deemed so useful to the United States military that it was classified as Restricted under the Espionage act [@wald1973]. 

```{r, include = FALSE}
gs <- function(proc, return = "n") {
  bs <- ifelse(proc == "asOF", "bsOF", "bsP")
 
  # Specify the design
  design <- getDesignGroupSequential(
    sided = 1,
    alpha = 0.05, 
    beta = 0.2,
    kMax = 3, 
    typeOfDesign = proc,
    typeBetaSpending= bs
    ) 
  
  # Get parameters
  parameters <- getSampleSizeMeans(design = design, groups = 2, alternative = 0.5)
  n_gs <- ceiling(parameters$maxNumberOfSubjects/3/2) # n per look per group
  alpha <- parameters$criticalValuesPValueScale
  futility <- parameters$futilityBoundsPValueScale %>% round(2)
  
  if(return == "alpha") {return(alpha %>% signif(2) %>% as.character() %>% sub(".", "", .,))}
  if(return == "n") {return(n_gs)}
  if(return == "futility") {return(futility %>% as.character() %>% sub(".", "", .,))}
}

n_p <- gs(proc = "asP")
alpha_p <- gs(proc = "asP", return = "alpha")
futility_p <- gs(proc = "asP", return = "futility")
n_of <- gs(proc = "asOF")
alpha_of <- gs(proc = "asOF", return = "alpha")
futility_of <- gs(proc = "asOF", return = "futility")
```

The most common procedure for sequential hypothesis testing is a group-sequential (GS) design [@proschan2006; @schonbrodt2017]. The goal of GS procedures is to efficiently reject a null hypothesis in favor of an alternative hypothesis. In a prototypical group-sequential procedure [e.g., the Pocock correction or the O'Brien-Fleming correction, @pocock1977; @fleming1984], a researcher is allowed to look at the data several times during the course of a study. 

It is well-known that multiple looks at data, conventionally, increase the type I error rate [@simmons2011]: i.e., the long-run rate at which a true null hypothesis is rejected. Sequential analysis procedures, however, allow a researcher to take multiple looks at data without inflating the type I error rate. Critical $\alpha$ levels are adjusted for each look so that the overall type I error rate is controlled at the desired level (for instance, at an $\alpha$ of 5%).

A general, sophisticated approach is provided by an "$\alpha$*-spending function*" [@lan1983; @wassmer2016]. This approach allows a researcher to have unequally spaced looks at their data (for instance, after the first 20% and the first 75% of the maximum sample size) and to adapt the number of looks as the experiment continues.

For example, a researcher who wishes to control their overall type I error rate at 5% and take *k* = 3 looks (i.e., two interim looks and one final look) at the data can use the common Pocock-like $\alpha$-spending function to determine what the corrected $\alpha$ level should be for each look (referred to as $\alpha_{1,k}$). In this case, $\alpha_{1,1:2}$ = `r alpha_p[1]` for the first two looks and $\alpha_{1,3}$ = `r alpha_p[3]` for the last look (i.e., for a one-tailed test). If during any of the interim looks $p \le \alpha_1$, the researcher can reject the null hypothesis and end the experiment; this is known as "*stopping for efficacy*". 

```{r, include = FALSE}
n_fixed <- ceiling(
  power.t.test(
    delta = 0.5, 
    sig.level = .05, 
    power = 0.8, 
    type = "two.sample", 
    alternative = "one.sided")$n)
```

Because the use of group-sequential procedures allows for early stopping, it reduces the average sample size needed. For example, if a researcher wants to conduct a one-sided, two-sample *t* test ($H_0:~\delta$ = 0 against $H_+:~\delta > 0$) with 80% power to detect a Cohen's *d* (i.e., a standardized mean difference) of 0.5, a conventional Null-Hypothesis Significance Test with an overall $\alpha$ of 5% would require `r n_fixed` participants per group. Using the Pocock-like GS design outlined above, the same study can be conducted with only `r n_p` participants per group per look.

In addition to stopping early for efficacy, a researcher can "*stop for futility*" if the observed effect size is too small to warrant further investigation. This can be done by manually specifying a lower bound (for example, a lower bound at *d* $\le$ 0 or, equivalently, at *p* $\ge$ .50) or by using a "$\beta$*-spending function*" to determine the critical lower bound for each look (referred to as $\alpha_{0,k}$).

The logic of the $\beta$-spending function is similar to that of the $\alpha$-spending function: critical values that warrant stopping for futility are determined a priori while controlling the overall type II error rate at the desired level (for instance, at a $\beta$ of 20%). In other words, the bounds are determined such that the probability of a type II error (i.e., given the number of subjects, the long-run probability of failing to reject the null when the true population effect $\delta$ = 0.5) is $\beta$ %. 

Figures 1A and 1B provide a visual overview of two possible group-sequential procedures (the Pocock-like and the O'Brien-Fleming-like designs) using $\alpha$- and $\beta$-spending functions, in the case of a one-tailed, two-sample *t*-test with $\alpha$ = 5%, $\beta$ = 20% for $\delta$ = 0.5, and $k_{max}$ = 3 looks. In our example of a Pocock-like design, the futility bounds are $\alpha_{0,1}$ = `r futility_p[1]` for the first look and $\alpha_{0,2}$ = `r futility_p[2]` for the second look. The researcher continues their study until the observed *p*-value crosses one of the prespecified $\alpha_1$ or $\alpha_0$ thresholds, or until the maximum *N* has been reached.  
  
Scientific experiments can require substantial time, money, and effort. At times, the well-being of research subjects, human or non-human, is at stake [@stefan2020]. Under the Ethics Code of the American Psychological Association, psychological scientists have the ethical obligation to minimize research subjects' exposure to physical, emotional, or psychological harm [@apa2020]. Sequential analysis can contribute to this goal, by allowing researchers to stop an experiment when the interim data show clear evidence that effects are positive, negative, or largely absent [@proschan2006]. Sequential hypothesis testing procedures, it turns out, are highly valuable tools for psychological scientists to add to their statistical toolbox. 

\newpage

## In-line code example

```{r}
n_fixed <- ceiling(
  power.t.test(
    delta = 0.5, 
    sig.level = .05, 
    power = 0.8, 
    type = "two.sample", 
    alternative = "one.sided")$n)
```

For example, if a researcher wants to conduct a one-sided, two-sample *t* test ($H_0:~\delta$ = 0 against $H_+:~\delta > 0$) with 80% power to detect a Cohen's *d* (i.e., a standardized mean difference) of 0.5, a conventional Null-Hypothesis Significance Test with an overall $\alpha$ of 5% would require `r n_fixed` participants per group.

## Acknowledgement
This manuscript was created using `r cite_r("r-references.bib")`.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

## Figure 1A
## *Pocock-like Group-Sequential Design: A worked example* 
\noindent
```{r, fig.heigth = 1, fig.width = 12}
knitr::include_graphics("figures/Figure1A.png", dpi = 1200)
```

*Note.* Pocock-like group-sequential design for a one-tailed, two-sample *t* test with $\alpha$ = .05, $\beta$ = .20 and $k_{max}$ = 3 looks. Critical values are determined on the basis of Pocock-like $\alpha$- and $\beta$-spending functions in *R* package *rpact* (Wassmer & Pahlke, 2020).

\newpage

## Figure 1B
## *O'Brien-Fleming-like Group-Sequential Design: A worked example* 
\noindent
```{r, fig.heigth = 1, fig.width = 12}
knitr::include_graphics("figures/Figure1B.png", dpi = 1200)
```

*Note.* O'Brien-Fleming-like group-sequential design for a one-tailed, two-sample *t* test with $\alpha$ = .05, $\beta$ = .20 and $k_{max}$ = 3 looks. Critical values are determined on the basis of O'Brien-Fleming-like $\alpha$- and $\beta$-spending functions in *R* package *rpact* (Wassmer & Pahlke, 2020).
